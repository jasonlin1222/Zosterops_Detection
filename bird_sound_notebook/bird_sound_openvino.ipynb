{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6973dca1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23576\\1608366977.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;34m'common/python'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mopenvino\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_zoo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDetectionModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDetectionWithLandmarks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRESIZE_TYPES\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOutputTransform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import logging as log\n",
    "import sys\n",
    "from argparse import ArgumentParser, SUPPRESS\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "\n",
    "import cv2\n",
    "\n",
    "sys.path.append(str(Path(__file__).resolve().parents[2] / 'common/python'))\n",
    "\n",
    "from openvino.model_zoo.model_api.models import DetectionModel, DetectionWithLandmarks, RESIZE_TYPES, OutputTransform\n",
    "from openvino.model_zoo.model_api.performance_metrics import PerformanceMetrics\n",
    "from openvino.model_zoo.model_api.pipelines import get_user_config, AsyncPipeline\n",
    "from openvino.model_zoo.model_api.adapters import create_core, OpenvinoAdapter, OVMSAdapter\n",
    "\n",
    "import monitors\n",
    "from images_capture import open_images_capture\n",
    "from helpers import resolution, log_latency_per_stage\n",
    "from visualizers import ColorPalette\n",
    "\n",
    "log.basicConfig(format='[ %(levelname)s ] %(message)s', level=log.DEBUG, stream=sys.stdout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3843b9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_argparser():\n",
    "    parser = ArgumentParser(add_help=False)\n",
    "    args = parser.add_argument_group('Options')\n",
    "    args.add_argument('-h', '--help', action='help', default=SUPPRESS, help='Show this help message and exit.')\n",
    "    args.add_argument('-m', '--model', required=True,\n",
    "                      help='Required. Path to an .xml file with a trained model '\n",
    "                           'or address of model inference service if using ovms adapter.')\n",
    "    available_model_wrappers = [name.lower() for name in DetectionModel.available_wrappers()]\n",
    "    args.add_argument('-at', '--architecture_type', help='Required. Specify model\\' architecture type.',\n",
    "                      type=str, required=True, choices=available_model_wrappers)\n",
    "    args.add_argument('--adapter', help='Optional. Specify the model adapter. Default is openvino.',\n",
    "                      default='openvino', type=str, choices=('openvino', 'ovms'))\n",
    "    args.add_argument('-i', '--input', required=True,\n",
    "                      help='Required. An input to process. The input must be a single image, '\n",
    "                           'a folder of images, video file or camera id.')\n",
    "    args.add_argument('-d', '--device', default='CPU', type=str,\n",
    "                      help='Optional. Specify the target device to infer on; CPU, GPU, HDDL or MYRIAD is '\n",
    "                           'acceptable. The demo will look for a suitable plugin for device specified. '\n",
    "                           'Default value is CPU.')\n",
    "\n",
    "    common_model_args = parser.add_argument_group('Common model options')\n",
    "    common_model_args.add_argument('--labels', help='Optional. Labels mapping file.', default=None, type=str)\n",
    "    common_model_args.add_argument('-t', '--prob_threshold', default=0.5, type=float,\n",
    "                                   help='Optional. Probability threshold for detections filtering.')\n",
    "    common_model_args.add_argument('--resize_type', default=None, choices=RESIZE_TYPES.keys(),\n",
    "                                   help='Optional. A resize type for model preprocess. By default used model predefined type.')\n",
    "    common_model_args.add_argument('--input_size', default=(600, 600), type=int, nargs=2,\n",
    "                                   help='Optional. The first image size used for CTPN model reshaping. '\n",
    "                                        'Default: 600 600. Note that submitted images should have the same resolution, '\n",
    "                                        'otherwise predictions might be incorrect.')\n",
    "    common_model_args.add_argument('--anchors', default=None, type=float, nargs='+',\n",
    "                                   help='Optional. A space separated list of anchors. '\n",
    "                                        'By default used default anchors for model. Only for YOLOV4 architecture type.')\n",
    "    common_model_args.add_argument('--masks', default=None, type=int, nargs='+',\n",
    "                                   help='Optional. A space separated list of mask for anchors. '\n",
    "                                        'By default used default masks for model. Only for YOLOV4 architecture type.')\n",
    "    common_model_args.add_argument('--layout', type=str, default=None,\n",
    "                                   help='Optional. Model inputs layouts. '\n",
    "                                        'Ex. NCHW or input0:NCHW,input1:NC in case of more than one input.')\n",
    "    common_model_args.add_argument('--num_classes', default=None, type=int,\n",
    "                                   help='Optional. Number of detected classes. Only for NanoDet, NanoDetPlus '\n",
    "                                        'architecture types.')\n",
    "\n",
    "    infer_args = parser.add_argument_group('Inference options')\n",
    "    infer_args.add_argument('-nireq', '--num_infer_requests', help='Optional. Number of infer requests',\n",
    "                            default=0, type=int)\n",
    "    infer_args.add_argument('-nstreams', '--num_streams',\n",
    "                            help='Optional. Number of streams to use for inference on the CPU or/and GPU in throughput '\n",
    "                                 'mode (for HETERO and MULTI device cases use format '\n",
    "                                 '<device1>:<nstreams1>,<device2>:<nstreams2> or just <nstreams>).',\n",
    "                            default='', type=str)\n",
    "    infer_args.add_argument('-nthreads', '--num_threads', default=None, type=int,\n",
    "                            help='Optional. Number of threads to use for inference on CPU (including HETERO cases).')\n",
    "\n",
    "    io_args = parser.add_argument_group('Input/output options')\n",
    "    io_args.add_argument('--loop', default=False, action='store_true',\n",
    "                         help='Optional. Enable reading the input in a loop.')\n",
    "    io_args.add_argument('-o', '--output', required=False,\n",
    "                         help='Optional. Name of the output file(s) to save.')\n",
    "    io_args.add_argument('-limit', '--output_limit', required=False, default=1000, type=int,\n",
    "                         help='Optional. Number of frames to store in output. '\n",
    "                              'If 0 is set, all frames are stored.')\n",
    "    io_args.add_argument('--no_show', help=\"Optional. Don't show output.\", action='store_true')\n",
    "    io_args.add_argument('--output_resolution', default=None, type=resolution,\n",
    "                         help='Optional. Specify the maximum output window resolution '\n",
    "                              'in (width x height) format. Example: 1280x720. '\n",
    "                              'Input frame size used by default.')\n",
    "    io_args.add_argument('-u', '--utilization_monitors', default='', type=str,\n",
    "                         help='Optional. List of monitors to show initially.')\n",
    "\n",
    "    input_transform_args = parser.add_argument_group('Input transform options')\n",
    "    input_transform_args.add_argument('--reverse_input_channels', default=False, action='store_true',\n",
    "                                      help='Optional. Switch the input channels order from '\n",
    "                                           'BGR to RGB.')\n",
    "    input_transform_args.add_argument('--mean_values', default=None, type=float, nargs=3,\n",
    "                                      help='Optional. Normalize input by subtracting the mean '\n",
    "                                           'values per channel. Example: 255.0 255.0 255.0')\n",
    "    input_transform_args.add_argument('--scale_values', default=None, type=float, nargs=3,\n",
    "                                      help='Optional. Divide input by scale values per channel. '\n",
    "                                           'Division is applied after mean values subtraction. '\n",
    "                                           'Example: 255.0 255.0 255.0')\n",
    "\n",
    "    debug_args = parser.add_argument_group('Debug options')\n",
    "    debug_args.add_argument('-r', '--raw_output_message', help='Optional. Output inference results raw values showing.',\n",
    "                            default=False, action='store_true')\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0ccc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_detections(frame, detections, palette, labels, output_transform):\n",
    "    frame = output_transform.resize(frame)\n",
    "    for detection in detections:\n",
    "        class_id = int(detection.id)\n",
    "        color = palette[class_id]\n",
    "        det_label = labels[class_id] if labels and len(labels) >= class_id else '#{}'.format(class_id)\n",
    "        xmin, ymin, xmax, ymax = detection.get_coords()\n",
    "        xmin, ymin, xmax, ymax = output_transform.scale([xmin, ymin, xmax, ymax])\n",
    "        cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), color, 2)\n",
    "        cv2.putText(frame, '{} {:.1%}'.format(det_label, detection.score),\n",
    "                    (xmin, ymin - 7), cv2.FONT_HERSHEY_COMPLEX, 0.6, color, 1)\n",
    "        if isinstance(detection, DetectionWithLandmarks):\n",
    "            for landmark in detection.landmarks:\n",
    "                landmark = output_transform.scale(landmark)\n",
    "                cv2.circle(frame, (int(landmark[0]), int(landmark[1])), 2, (0, 255, 255), 2)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007da69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_raw_results(detections, labels, frame_id):\n",
    "    log.debug(' ------------------- Frame # {} ------------------ '.format(frame_id))\n",
    "    log.debug(' Class ID | Confidence | XMIN | YMIN | XMAX | YMAX ')\n",
    "    for detection in detections:\n",
    "        xmin, ymin, xmax, ymax = detection.get_coords()\n",
    "        class_id = int(detection.id)\n",
    "        det_label = labels[class_id] if labels and len(labels) >= class_id else '#{}'.format(class_id)\n",
    "        log.debug('{:^9} | {:10f} | {:4} | {:4} | {:4} | {:4} '\n",
    "                  .format(det_label, detection.score, xmin, ymin, xmax, ymax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4caa338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    args = build_argparser().parse_args()\n",
    "    if args.architecture_type != 'yolov4' and args.anchors:\n",
    "        log.warning('The \"--anchors\" option works only for \"-at==yolov4\". Option will be omitted')\n",
    "    if args.architecture_type != 'yolov4' and args.masks:\n",
    "        log.warning('The \"--masks\" option works only for \"-at==yolov4\". Option will be omitted')\n",
    "    if args.architecture_type not in ['nanodet', 'nanodet-plus'] and args.num_classes:\n",
    "        log.warning('The \"--num_classes\" option works only for \"-at==nanodet\" and \"-at==nanodet-plus\". Option will be omitted')\n",
    "\n",
    "    cap = open_images_capture(args.input, args.loop)\n",
    "\n",
    "    if args.adapter == 'openvino':\n",
    "        plugin_config = get_user_config(args.device, args.num_streams, args.num_threads)\n",
    "        model_adapter = OpenvinoAdapter(create_core(), args.model, device=args.device, plugin_config=plugin_config,\n",
    "                                        max_num_requests=args.num_infer_requests, model_parameters = {'input_layouts': args.layout})\n",
    "    elif args.adapter == 'ovms':\n",
    "        model_adapter = OVMSAdapter(args.model)\n",
    "\n",
    "    configuration = {\n",
    "        'resize_type': args.resize_type,\n",
    "        'mean_values': args.mean_values,\n",
    "        'scale_values': args.scale_values,\n",
    "        'reverse_input_channels': args.reverse_input_channels,\n",
    "        'path_to_labels': args.labels,\n",
    "        'confidence_threshold': args.prob_threshold,\n",
    "        'input_size': args.input_size, # The CTPN specific\n",
    "        'num_classes': args.num_classes, # The NanoDet and NanoDetPlus specific\n",
    "    }\n",
    "    model = DetectionModel.create_model(args.architecture_type, model_adapter, configuration)\n",
    "    model.log_layers_info()\n",
    "\n",
    "    detector_pipeline = AsyncPipeline(model)\n",
    "\n",
    "    next_frame_id = 0\n",
    "    next_frame_id_to_show = 0\n",
    "\n",
    "    palette = ColorPalette(len(model.labels) if model.labels else 100)\n",
    "    metrics = PerformanceMetrics()\n",
    "    render_metrics = PerformanceMetrics()\n",
    "    presenter = None\n",
    "    output_transform = None\n",
    "    video_writer = cv2.VideoWriter()\n",
    "\n",
    "    while True:\n",
    "        if detector_pipeline.callback_exceptions:\n",
    "            raise detector_pipeline.callback_exceptions[0]\n",
    "        # Process all completed requests\n",
    "        results = detector_pipeline.get_result(next_frame_id_to_show)\n",
    "        if results:\n",
    "            objects, frame_meta = results\n",
    "            frame = frame_meta['frame']\n",
    "            start_time = frame_meta['start_time']\n",
    "\n",
    "            if len(objects) and args.raw_output_message:\n",
    "                print_raw_results(objects, model.labels, next_frame_id_to_show)\n",
    "\n",
    "            presenter.drawGraphs(frame)\n",
    "            rendering_start_time = perf_counter()\n",
    "            frame = draw_detections(frame, objects, palette, model.labels, output_transform)\n",
    "            render_metrics.update(rendering_start_time)\n",
    "            metrics.update(start_time, frame)\n",
    "\n",
    "            if video_writer.isOpened() and (args.output_limit <= 0 or next_frame_id_to_show <= args.output_limit-1):\n",
    "                video_writer.write(frame)\n",
    "            next_frame_id_to_show += 1\n",
    "\n",
    "            if not args.no_show:\n",
    "                cv2.imshow('Detection Results', frame)\n",
    "                key = cv2.waitKey(1)\n",
    "\n",
    "                ESC_KEY = 27\n",
    "                # Quit.\n",
    "                if key in {ord('q'), ord('Q'), ESC_KEY}:\n",
    "                    break\n",
    "                presenter.handleKey(key)\n",
    "            continue\n",
    "\n",
    "        if detector_pipeline.is_ready():\n",
    "            # Get new image/frame\n",
    "            start_time = perf_counter()\n",
    "            frame = cap.read()\n",
    "            if frame is None:\n",
    "                if next_frame_id == 0:\n",
    "                    raise ValueError(\"Can't read an image from the input\")\n",
    "                break\n",
    "            if next_frame_id == 0:\n",
    "                output_transform = OutputTransform(frame.shape[:2], args.output_resolution)\n",
    "                if args.output_resolution:\n",
    "                    output_resolution = output_transform.new_resolution\n",
    "                else:\n",
    "                    output_resolution = (frame.shape[1], frame.shape[0])\n",
    "                presenter = monitors.Presenter(args.utilization_monitors, 55,\n",
    "                                               (round(output_resolution[0] / 4), round(output_resolution[1] / 8)))\n",
    "                if args.output and not video_writer.open(args.output, cv2.VideoWriter_fourcc(*'MJPG'),\n",
    "                                                         cap.fps(), output_resolution):\n",
    "                    raise RuntimeError(\"Can't open video writer\")\n",
    "            # Submit for inference\n",
    "            detector_pipeline.submit_data(frame, next_frame_id, {'frame': frame, 'start_time': start_time})\n",
    "            next_frame_id += 1\n",
    "        else:\n",
    "            # Wait for empty request\n",
    "            detector_pipeline.await_any()\n",
    "\n",
    "    detector_pipeline.await_all()\n",
    "    if detector_pipeline.callback_exceptions:\n",
    "        raise detector_pipeline.callback_exceptions[0]\n",
    "    # Process completed requests\n",
    "    for next_frame_id_to_show in range(next_frame_id_to_show, next_frame_id):\n",
    "        results = detector_pipeline.get_result(next_frame_id_to_show)\n",
    "        objects, frame_meta = results\n",
    "        frame = frame_meta['frame']\n",
    "        start_time = frame_meta['start_time']\n",
    "\n",
    "        if len(objects) and args.raw_output_message:\n",
    "            print_raw_results(objects, model.labels, next_frame_id_to_show)\n",
    "\n",
    "        presenter.drawGraphs(frame)\n",
    "        rendering_start_time = perf_counter()\n",
    "        frame = draw_detections(frame, objects, palette, model.labels, output_transform)\n",
    "        render_metrics.update(rendering_start_time)\n",
    "        metrics.update(start_time, frame)\n",
    "\n",
    "        if video_writer.isOpened() and (args.output_limit <= 0 or next_frame_id_to_show <= args.output_limit-1):\n",
    "            video_writer.write(frame)\n",
    "\n",
    "        if not args.no_show:\n",
    "            cv2.imshow('Detection Results', frame)\n",
    "            key = cv2.waitKey(1)\n",
    "\n",
    "            ESC_KEY = 27\n",
    "            # Quit.\n",
    "            if key in {ord('q'), ord('Q'), ESC_KEY}:\n",
    "                break\n",
    "            presenter.handleKey(key)\n",
    "\n",
    "    metrics.log_total()\n",
    "    log_latency_per_stage(cap.reader_metrics.get_latency(),\n",
    "                          detector_pipeline.preprocess_metrics.get_latency(),\n",
    "                          detector_pipeline.inference_metrics.get_latency(),\n",
    "                          detector_pipeline.postprocess_metrics.get_latency(),\n",
    "                          render_metrics.get_latency())\n",
    "    for rep in presenter.reportMeans():\n",
    "        log.info(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5832c8aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
